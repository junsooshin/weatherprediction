{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Max Temperature using Machine Learning\n",
    "*Junsoo Derek Shin*\n",
    "<br>\n",
    "*May 2018*\n",
    "***\n",
    "### Purpose\n",
    "The purpose of the project is to predict max temperatures of the days in April 2018 using machine learning techniques.\n",
    "\n",
    "The general guide on the machine learning concepts and the inspiration to apply them came from came from these two sources:\n",
    "1. https://www.kaggle.com/learn/machine-learning\n",
    "2. http://stackabuse.com/using-machine-learning-to-predict-the-weather-part-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get and Load Weather Underground Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`extract_weather_data()`** function asks the Weather Underground API for the historic data and writes the returned JSON data into a text file. The delay is put in, so that the number of requests doesn't exceed the 10-requests-per-minute limit. There is also daily limit of 500 requests, so `days` argument should not be greater than 500. I have already run this function and gathered data from January 1, 2014 to April 30, 2018, and the data is available in the `weatherdata.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import json\n",
    "\n",
    "from collections import namedtuple\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get history JSON data from the Weather Underground API\n",
    "# def extract_weather_data(api_key, base_url, target_date, days):\n",
    "#     with open('weatherdata.txt', 'a') as outfile:\n",
    "#         for _ in range(days):\n",
    "#             request = base_url.format(api_key, target_date.strftime('%Y%m%d'))\n",
    "#             response = requests.get(request)\n",
    "#             if response.status_code == 200:\n",
    "#                 data = response.json()[\"history\"][\"dailysummary\"][0]\n",
    "#                 json.dump(data, outfile)\n",
    "#                 outfile.write('\\n')\n",
    "#             time.sleep(6)\n",
    "#             target_date += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`fill_dateframe()`** function reads the text file filled with JSON objects and creates a Pandas DataFrame from it. The `target_date` argument is the starting date of this weather text file and should be a `datetime` object like `datetime(2014, 1, 1)`. This date increments as we create the DataFrame and works as the index. The `namedtuple` is similar to a struct or class and lets us use attributes, so that the code is more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from the text file of JSON objects, create a list of namedtuples, and use it\n",
    "# to create a DataFrame\n",
    "def fill_dataframe(target_date):\n",
    "    features = [\"date\", \"meantempm\", \"meandewptm\", \"meanpressurem\", \n",
    "                \"maxhumidity\", \"minhumidity\", \"maxtempm\", \"mintempm\", \n",
    "                \"maxdewptm\", \"mindewptm\", \"maxpressurem\", \"minpressurem\", \n",
    "                \"precipm\"]\n",
    "    DailySummary = namedtuple(\"DailySummary\", features)\n",
    "    records = []\n",
    "    with open('weatherdata.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            records.append(DailySummary(\n",
    "                date = target_date,\n",
    "                meantempm = data[\"meantempm\"],\n",
    "                meandewptm = data[\"meandewptm\"],\n",
    "                meanpressurem = data[\"meanpressurem\"],\n",
    "                maxhumidity = data[\"maxhumidity\"],\n",
    "                minhumidity = data[\"minhumidity\"],\n",
    "                maxtempm = data[\"maxtempm\"],\n",
    "                mintempm = data[\"mintempm\"],\n",
    "                maxdewptm = data[\"maxdewptm\"],\n",
    "                mindewptm = data[\"mindewptm\"],\n",
    "                maxpressurem = data[\"maxpressurem\"],\n",
    "                minpressurem = data[\"minpressurem\"],\n",
    "                precipm = data[\"precipm\"],\n",
    "            ))\n",
    "            target_date += timedelta(days=1)\n",
    "    df = pd.DataFrame(records, columns=features).set_index('date')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_date = datetime(2014, 1, 1)\n",
    "days = 365\n",
    "# extract_weather_data(API_KEY, BASE_URL, target_date, days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'weatherdata2.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-79ee2948d968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-fc6abcd56cf4>\u001b[0m in \u001b[0;36mfill_dataframe\u001b[0;34m(target_date)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mDailySummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DailySummary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weatherdata2.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'weatherdata2.txt'"
     ]
    }
   ],
   "source": [
    "data = fill_dataframe(target_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the columns are `object` (or `string`). Let's convert them into numeric data, so that it's easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['precipm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`precipm` column was the only column that couldn't be converted into numbers, and the reason was that it had values such as `'T'`, which stands for \"Trace\" or a very litte amount of precipitation. Since \"Trace\" should be different from zero precipitation, I will assign an arbitrary value of 0.01 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.apply(pd.to_numeric, errors='coerce')\n",
    "trace_rows = data['precipm'].isnull()\n",
    "data.loc[trace_rows, 'precipm'] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['precipm'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add Features/Columns\n",
    "What we want to predict is clear: it's `maxtempm`. However, we cannot use the data from the same date to train our model because we won't have those data from that day. So, one way to go about this is adding columns of measurements from the previous days. For example, for `meanpressurem`, we would have columns, such as `meanpressurem_1`, `meanpressurem_2`, `meanpressurem_3`, which are measurements from 1, 2 and 3 days prior.\n",
    "\n",
    "So let's add the columns for 1, 2 and 3-days-prior measurements for every column except for the temperature columns. For the first few rows at the top, we need a buffer because we don't have the prior-days data for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# given a feature and the number of prior days, add column(s) to the DataFrame\n",
    "def derive_nth_day_feature(df, feature, N):\n",
    "    num_rows = df.shape[0]\n",
    "    nth_prior_measurements = [None]*N + [df[feature][i] for i in range(0, num_rows-N)]\n",
    "    col_name = \"{}_{}\".format(feature, N)\n",
    "    df[col_name] = nth_prior_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for feature in data.columns:\n",
    "    for N in range(1, 4):\n",
    "        derive_nth_day_feature(data, feature, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need the measurements on the same day as the temperatures because we won't have them for the days we are trying to predict. If we can have those measurements, we can probably have temperatures for those days as well. So let's drop those columns.\n",
    "\n",
    "We also see that the columns are missing certain amount of values because of the missing n_day prior values. Other than those rows, everything seems to be filled in. Although every row is valuable, I don't want to impute values that could be way off, let's drop those 1-to-3 rows and obtain non-null data all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop = ['meandewptm', 'maxdewptm', 'mindewptm',\n",
    "                    'meanpressurem', 'maxpressurem', 'minpressurem',\n",
    "                    'maxhumidity', 'minhumidity',\n",
    "                    'precipm']\n",
    "data.drop(features_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into **`train_data`** and **`april_data`**, so that we can train our models with **`train_data`** and eventually test them with **`april_data`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "april_data = data[data.index >= datetime(2018, 4, 1)]\n",
    "train_data = data.drop(april_data.index, axis=0)\n",
    "april_data.info()\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Linear Regression\n",
    "Now we can build our first model using linear regression! Linear regression requires that the features we are using and the target variable we are trying to predict have linear relationships. One way to assess that is by calculating Pearson correlation coefficients. These values range from -1 to 1, and the values close to -1 and 1 mean that the features have strong linear relationships with the target variable, and values close to 0 mean that they have weak linear relationships with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data.corr()[['maxtempm']].sort_values('maxtempm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the features with temperature and dew point have linear relationships with max temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_features = [feature for feature in train_data.columns\n",
    "                                       if 'temp' in feature or\n",
    "                                          'dewpt' in feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model will use linear regression. It will train on 80% of the data and tested on the 20% of the data. Using cross validation, we will cut the data in five, rotate the training-testing-data pairs, and then calculate the mean of those five scores, which will be the final score for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = train_data[linear_features].drop(['meantempm', 'maxtempm', 'mintempm'], axis=1)\n",
    "y = train_data['maxtempm']\n",
    "\n",
    "scores = -1 * cross_val_score(LinearRegression(),\n",
    "                              X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "prev_maxtemp = X['maxtempm_1']\n",
    "mean_absolute_error(y, prev_maxtemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the mean error from the linear regression model is slightly better than just using the max temperatures from a day before, the error of 3.42 degrees Celsius (6.16 degrees Fahrenheit) is pretty high. Let's see if we can improve our predictions with another technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Decision Tree, Random Forest and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_data.drop(['meantempm', 'maxtempm', 'mintempm'], axis=1)\n",
    "y = train_data['maxtempm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    scores = -1 * cross_val_score(DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes),\n",
    "                            X, y,\n",
    "                            cv=5,\n",
    "                            scoring='neg_mean_absolute_error')\n",
    "    print(str(max_leaf_nodes), str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "scores = -1 * cross_val_score(RandomForestRegressor(50),\n",
    "                              X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "scores = -1 * cross_val_score(XGBRegressor(n_estimators=1000, learning_rate=0.05),\n",
    "                              X, y,\n",
    "                              cv=5,\n",
    "                              scoring='neg_mean_absolute_error')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predict April 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the models trained and tested on the 3 years of data, the linear regression model seems to be the most accurate model, so we will use the linear regression model to predict the daily maximum temperatures of April 2018. First, let's train the model using the entire 3 years of data (not just 80% of them) since the April data will be our test data anyway. Then using the April data, we will make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y = train_data['maxtempm']\n",
    "train_X = train_data[linear_features].drop(['meantempm', 'maxtempm', 'mintempm'], axis=1)\n",
    "test_y = april_data['maxtempm']\n",
    "test_X = april_data[linear_features].drop(['meantempm', 'maxtempm', 'mintempm'], axis=1)\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(train_X, train_y)\n",
    "\n",
    "predictions = linear_model.predict(test_X)\n",
    "print(\"Linear Regression model's mean absolute error: \"+ str(mean_absolute_error(test_y, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create two naive benchmarks. The first benchmark will use previous-day measurements as its predictions. The second benchmark will use previous-year measurements as its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev_day_benchmark = april_data['maxtempm_1']\n",
    "prev_year_benchmark = train_data.loc[(train_data.index >= datetime(2017, 4, 1)) & \n",
    "                                     (train_data.index <= datetime(2017, 4, 30))]['maxtempm']\n",
    "print(\"prev_day MAE: \" + str(mean_absolute_error(test_y, prev_day_benchmark)))\n",
    "print(\"prev_year MAE: \" + str(mean_absolute_error(test_y, prev_year_benchmark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "April 2018 was a tough month to predict maximum temperatures. Maximum temperatures seemed to have changed day-to-day, and they were even more different from those of April 2017. Both benchmarks and our linear regression model performed with, I would say, large errors. However, the linear regression model does do better than the naive benchmarks, so this is a decent start. After studying more about the machine learning algorithms, I want to figure out what made the algorithms I used here struggled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Time to See What Happened"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how my predictions and the actual maximum temperatures look on a scatterplot. If the predicitons were good, the plot would show a straight, diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.scatter(predictions, test_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot looks more like two diagonal lines, and really not a straight line that I wanted.\n",
    "\n",
    "Let's check if there were any outliers in my training data that I missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_used = [feature for feature in linear_features \n",
    "                                     if feature not in ['meantempm', 'maxtempm', 'mintempm']]\n",
    "for feature in features_used:\n",
    "    train_data[feature].hist()\n",
    "    plt.xlabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be many outliers in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is true that using the temperature and dew point measurements in the 3 previous days to predict the maximum temperature for the next day seemed difficult during the training of the model as well. Perhaps the model should have been more strongly fit (maybe it's underfit at the moment), or perhaps the model should have been trained on the April data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_preds = linear_model.predict(train_X)\n",
    "print(str(mean_absolute_error(train_y, my_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(my_preds, train_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
